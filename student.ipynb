{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Introduction\n",
    "## 1.1 Background\n",
    "\n",
    "The real estate industry thrives on a foundation of accurate property valuations and market analysis. In dynamic markets characterized by fierce competition, real estate agents require reliable tools to determine optimal listing prices, attract buyers quickly, and maximize profits for their sellers. The real estate market is highly competitive. Pricing homes accurately is essential for attracting buyers, maximizing profits for sellers, and ensuring timely sales Traditionally, agents may rely heavily on recent comparable sales and their own experience, which can introduce subjectivity and potential for pricing errors. \n",
    "\n",
    "Additionally, limited availability of housing inventory, particularly in desirable neighborhoods or regions with high demand, can lead to increased buyer competition and inflated prices. This shortage may also result in longer wait times for buyers to find suitable properties. Meeting the diverse needs and preferences of clients, including first-time homebuyers, investors, and downsizing retirees, requires housing agents to have a deep understanding of market trends, property features, and financing options. \n",
    "\n",
    "Addressing these challenges requires collaboration and innovation within the real estate industry and proactive measures to promote affordability, fairness, and sustainability in housing markets. This project aims to provide agents with a data-driven tool to refine pricing strategies, highlight a property's most valuable assets, and offer informed recommendations for potential value-boosting renovations. \n",
    "\n",
    "A data-driven approach to property valuation can offer agents a significant edge, enabling them to make informed decisions based on market trends and property characteristics, ultimately leading to successful transactions for all parties involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Problem Statement\n",
    "The real estate market in the King County region faces challenges in accurately pricing homes, understanding the factors driving property values, and providing targeted renovation advice to homeowners. Traditional valuation methods may lack precision and fail to account for the diverse range of features influencing home prices. Consequently, real estate agents may struggle to offer accurate pricing estimates and relevant advice to clients, leading to suboptimal outcomes for both buyers and sellers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Aim of Project\n",
    "This project aims to develop data-driven models to support real estate agents in the King County region with accurate property pricing and targeted insights for client consultations. Specifically, the project will:\n",
    "\n",
    "i. \n",
    " Create a model for house price prediction: Provide price predictions for potential listings based on key property characteristics.\n",
    "\n",
    "ii.\n",
    " Create a model for price range prediction: Establish realistic price ranges for properties based on their features, enhancing agents' negotiation strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Main Objective\n",
    "Empower real estate agents with data-backed  pricing tools to optimize listing strategies, improve  client communication, and maximize seller outcomes.\n",
    "\n",
    "### 1.5 Other Objectives\n",
    "i)\tDevelop a multiple linear regression model using the King County Housing dataset to predict home prices based on various features accurately.\n",
    "ii)\tProvide actionable insights to real estate agents to assist them in pricing homes accurately, understanding factors influencing property values, and advising homeowners on targeted renovations.\n",
    "iii)\tUnderstanding the features that have the most significant impact on home prices for effective marketing and negotiation strategies.\n",
    "\n",
    "### 1.6. Business and Data Understanding\n",
    "#### 1.6.1 Stakeholder\n",
    "Real estate agents in King County face a competitive market where accurate property valuations are essential for success.  This project aims to address these challenges by developing data-driven models that will equip agents with the following:\n",
    "\n",
    "#### 1.6.2 Business Needs\n",
    "* Competitive Pricing: The price prediction model will provide objective estimates of a property's fair market value,  helping agents establish initial listing prices that are competitive yet realistic.  This will attract qualified buyers quickly and minimize the time a property sits on the market.  Additionally, the model's insights can inform negotiation strategies, empowering agents to make data-supported decisions throughout the selling process.\n",
    "\n",
    "* Understanding Value Drivers: By analyzing the impact of various housing features on predicted prices, the models will shed light on which characteristics are most desired by buyers in the King County market.  This will allow agents to identify a property's strengths and potential areas for improvement.  For instance, the model might reveal that a property with a large backyard is likely to command a higher price than one without.  Armed with this knowledge, agents can effectively highlight a property's most valuable assets in marketing materials and during client consultations.\n",
    "\n",
    "* Client Communication: Data-driven insights can significantly enhance communication and build trust with clients.  Agents can leverage the model's predictions and analysis of value drivers to provide sellers with clear explanations of the pricing strategy and recommendations for optimizations.  This fosters transparency and empowers sellers to make informed decisions throughout the listing process.\n",
    "\n",
    "To address these challenges, this project aims to leverage regression analysis on the King County Housing dataset. By developing a robust regression model, we seek to identify the key drivers of property value and provide real estate agents with actionable insights for pricing homes accurately and advising homeowners on strategic renovations. Our objective is to empower real estate agents with data-driven tools and knowledge to enhance their decision-making process and ultimately improve customer satisfaction and trust in the real estate market.\n",
    "\n",
    "### 1.7 Methodology\n",
    "\n",
    "#### 1.7.1 Dataset\n",
    "King County House Sales dataset (kc_house_data.csv).\n",
    "\n",
    "#### 1.7.2 Statistical Approach\n",
    "Multiple linear regression is a well-established statistical technique for modeling continuous relationships between a dependent variable (in our case, house price) and multiple independent variables (such as square footage, number of bedrooms, and waterfront location). By analyzing the historical sales data in the King County dataset, the model will learn the weights (coefficients) of each feature's influence on price. This allows the model to generate a prediction for the price of a new house based on its specific characteristics. \n",
    "\n",
    "#### 1.8 Features (Columns) Used and Their Relevance:\n",
    "\n",
    "* id: Unique identifier for each house sale record. May not be directly used for modeling, but essential for data cleaning and reference.\n",
    "* date: Date of the house sale. Useful for time-based analysis, filtering by timeframe, or creating features related to seasonality.\n",
    "* price: The target variable â€“ the outcome we aim to predict.\n",
    "* bedrooms: Number of bedrooms, essential for accommodating buyer needs.\n",
    "* bathrooms: Number of bathrooms, impacting convenience and value.\n",
    "* sqft_living: Square footage of interior living space, a major price driver.\n",
    "* sqft_lot: Square footage of the land parcel, affecting lot size and potential use.\n",
    "* floors: Number of floors in the house, a possible indicator of layout and space.\n",
    "* waterfront: Binary variable indicating whether the property has waterfront access, a highly desirable feature in the region.\n",
    "* view: Rated view quality of the property, a potential value-adding aspect.\n",
    "* condition: Overall condition of the house, likely affecting price and renovation needs.\n",
    "* grade: Overall grade assigned to the housing unit based on King County grading system. Understanding the details of this grading system is crucial.\n",
    "* sqft_above: Square footage of the house excluding the basement.\n",
    "* sqft_basement: Square footage of the basement, if present.\n",
    "* yr_built: Year the house was originally built, indicating age.\n",
    "* yr_renovated: Year of the last renovation, if applicable. Influences condition and potential for further updates.\n",
    "* zipcode: Geographic location, potentially related to market dynamics and neighborhood desirability.\n",
    "* lat: Latitude coordinate, useful for mapping or finer-grained location analysis.\n",
    "* long: Longitude coordinate, used in conjunction with latitude.\n",
    "* sqft_living15: Living space of homes in the neighborhood (15 nearest neighbors). Can provide insight into local market comparisons.\n",
    "* sqft_lot15: Lot size of homes in the neighborhood (15 nearest neighbors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Data Prepartion\n",
    "### 2.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are importing several libraries that we will use throughout this notebook. Libraries like `numpy` and `pandas` are fundamental for data manipulation and analysis. `matplotlib` and `seaborn` are used for data visualization.`sklearn` provides tools for data mining and data analysis, including model selection and linear regression, and `missingno` offers a convenient way to visualize missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'missingno'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmissingno\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmsno\u001b[39;00m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'missingno'"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import missingno as msno \n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Overview\n",
    "In this section, we will load the real estate data from a CSV file and perform some initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Display the last few rows of the DataFrame\n",
    "print(df.tail())\n",
    "\n",
    "# Describe the data\n",
    "print(df.describe().transpose())\n",
    "\n",
    "# Provide info about the data\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize missing data\n",
    "msno.bar(df)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "print(f\"Number of duplicate rows = {duplicates.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Examine Unique Values\n",
    "In the following cell, we are defining a function to inspect each column for unique values in our dataset. The inspect_columns function iterates over each column in the DataFrame and prints the unique values. This is a useful way to quickly see the range or categories of values that each feature can take. After defining this function, we use it to inspect each column in our dataset. This can help us identify any anomalies or inconsistencies in our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect each column for unique values\n",
    "def inspect_columns(df):\n",
    "    \"\"\"Prints unique values of each column in the DataFrame.\"\"\"\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: \\n{df[col].unique()}\\n\")\n",
    "\n",
    "# Inspect each column for unique values\n",
    "inspect_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "Data Types: The dataset contains a variety of data types. We have integer values (such as 'id', 'bedrooms', 'floors'), floating-point numbers (like 'price', 'bathrooms', 'sqft_living'), and string or object types (including 'date', 'waterfront', and possibly others). These different types will need to be handled appropriately during the data preprocessing stage before we can use them in a machine learning model.\n",
    "\n",
    "Missing Values: There appears to be missing data in the 'view', 'waterfront', and 'yr_renovated' columns, as suggested by their lower count. We may also have missing data in other features. It will be important to identify and handle these missing values appropriately, either by imputation or by excluding the affected records, depending on the nature and extent of the missingness.\n",
    "\n",
    "Adding New Columns: We could potentially enhance our dataset by creating new features based on the existing ones. These new features could capture important information in a different way or represent interactions between existing features, which could help improve the performance of our model.\n",
    "\n",
    "Excluding Some Columns: The 'id' column appears to be a unique identifier for each record. While it's unlikely to be a useful predictor for 'price', it's important to retain it for reference purposes. The 'zipcode', 'lat', 'long', 'sqft_living15', and 'sqft_lot15' columns provide valuable information about the location of each property, which is known to be a strong predictor of price. However, we should consider dropping the 'zipcode' column to prevent our model from becoming too specific to the training data, which could lead to overfitting and reduce its ability to generalize to new properties in unseen zip codes.\n",
    "\n",
    "Outliers: There are potential outliers in features like 'sqft_living', 'sqft_lot', 'bedrooms', and possibly 'price', as indicated by the large differences between the minimum and maximum values and the quartiles. These outliers could skew our model and should be handled appropriately, either by transformation, binning, or exclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Data Preprocessing, Manipulation and Cleaning\n",
    "\n",
    "### 3.1 Data Types and Initial Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are performing several data preprocessing steps to prepare our dataset for analysis. The 'date' column is converted into the correct datetime format for easier manipulation and analysis. Categorical variables such as 'view', 'condition', 'grade', and 'waterfront' are encoded into numerical values to facilitate their use in machine learning models. The 'sqft_basement' column is converted to integer type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert date into correct format\n",
    "def convert_date(df, date_column):\n",
    "    \"\"\"Converts a date column into the correct format.\"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], format='%m/%d/%Y')\n",
    "\n",
    "# Convert date into correct format\n",
    "convert_date(df, 'date')\n",
    "\n",
    "# Function to carry out encoding\n",
    "def encode_columns(df):\n",
    "    \"\"\"Encodes categorical variables.\"\"\"\n",
    "    view_mapping = {'none': 0, 'good': 3, 'excellent': 4, 'average': 1, 'fair': 2}\n",
    "    condition_mapping = {'average': 2, 'very good': 4, 'good': 3, 'poor': 0, 'fair': 1}\n",
    "    grade_mapping = {'7 average': 7, '6 low average': 6, '8 good': 8, '11 excellent': 11, '9 better': 9, '5 fair': 5,\n",
    "                     '10 very good': 10, '12 luxury': 12, '4 low': 4, '3 poor': 3, '13 mansion': 13}\n",
    "    waterfront_mapping = {'no': 0, 'yes': 1}\n",
    "    df['view'] = df['view'].str.lower().str.strip().map(view_mapping)\n",
    "    df['condition'] = df['condition'].str.lower().str.strip().map(condition_mapping)\n",
    "    df['grade'] = df['grade'].str.lower().str.strip().map(grade_mapping)\n",
    "    df['waterfront'] = df['waterfront'].str.lower().str.strip().map(waterfront_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# Convert date into correct format\n",
    "convert_date(df, 'date')\n",
    "\n",
    "# Encode categorical variables\n",
    "encode_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we are handling missing values in our dataset. We perform the following operations:\n",
    "1. Drop rows where the 'waterfront' or 'view' columns have missing values.\n",
    "2. For the 'yr_renovated' column, replace missing or zero values with the corresponding 'yr_built' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Drops rows with missing values in the 'waterfront', 'view', and 'sqft_basement' columns and replaces missing or 0 values in 'yr_renovated' with 'yr_built' values.\"\"\"\n",
    "    \n",
    "    # Drop rows with missing 'waterfront' values\n",
    "    df = df.dropna(subset=['waterfront'])\n",
    "\n",
    "    # Drop rows with missing 'view' values\n",
    "    df = df.dropna(subset=['view'])\n",
    "\n",
    "    # Drop rows with '?' values in 'sqft_basement'\n",
    "    df = df[df['sqft_basement'] != '?']\n",
    "\n",
    "    # Convert 'sqft_basement' to float before converting it to int\n",
    "    df['sqft_basement'] = df['sqft_basement'].astype(float).astype(int)\n",
    "\n",
    "    # Convert 'sqft_basement' to int\n",
    "    df['sqft_basement'] = df['sqft_basement'].astype(int)\n",
    "\n",
    "    # Replace missing or 0 values in 'yr_renovated' with 'yr_built' values\n",
    "    df.loc[df['yr_renovated'].isna() | (df['yr_renovated'] == 0), 'yr_renovated'] = df['yr_built']\n",
    "\n",
    "    return df\n",
    "# Handle missing values\n",
    "df = handle_missing_values(df)\n",
    "\n",
    "# Check for missing values in the dataframe \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Add New Columns\n",
    "\n",
    "In the following cell, we are enhancing our dataset by adding new columns that could provide more insights for our analysis.\n",
    "\n",
    "'age_in_2016' represents the age of the house in 2016.\n",
    "'renovation_age_in_2016' indicates the age of the renovation in 2016, if the house was renovated.\n",
    "'renovated' is a binary column indicating whether the house was renovated or not.\n",
    "'year' and 'month' are extracted from the 'date' column to provide temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_columns(df):\n",
    "    \"\"\"Adds new columns 'year', 'month', 'house_age', 'renovation_age', and 'season'.\"\"\"\n",
    "\n",
    "    # Year and month from 'date' column\n",
    "    df['year_sold'] = pd.DatetimeIndex(df['date']).year\n",
    "    df['month_sold'] = pd.DatetimeIndex(df['date']).month\n",
    "\n",
    "    # Calculate age of the house when sold\n",
    "    df['house_age'] = df['year_sold'] - df['yr_built']\n",
    "\n",
    "    # Calculate years since renovation (if renovated)\n",
    "    df['renovation_age'] = df['year_sold'] - df['yr_renovated'].fillna(df['year_sold'])\n",
    "\n",
    "    # Whether renovated or not\n",
    "    df['renovated'] = (df['yr_renovated'] - df['yr_built']).apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # Add 'season' column\n",
    "    df['season'] = df['month_sold'].apply(lambda month: (month%12 + 3)//3) # 1: Winter, 2: Spring, 3: Summer, 4: Fall\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add new columns\n",
    "df = add_new_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are creating and encoding group columns for 'yr_built' and 'price'. \n",
    "\n",
    "1. 'yr_built_group' categorizes the year the house was built into time periods.\n",
    "2. 'price_group' categorizes the price of the house into different ranges.\n",
    "3. 'yr_built_group_encoded' and 'price_group_encoded' are the encoded versions of the above categories, which can be useful for machine learning models.\n",
    "\n",
    "#### Year Built Encoding\n",
    "\n",
    "| Original 'yr_built' Range | Encoded Value ('yr_built_group') |\n",
    "|----------------------------|-----------------------------------|\n",
    "| Any year before 1900       | 0                                 |\n",
    "| 1900 - 1920                | 1                                 |\n",
    "| 1921 - 1940                | 2                                 |\n",
    "| 1941 - 1960                | 3                                 |\n",
    "| 1961 - 1980                | 4                                 |\n",
    "| 1981 - 2000                | 5                                 |\n",
    "| 2001 - 2020                | 6                                 |\n",
    "\n",
    "#### Price Encoding\n",
    "\n",
    "| Original 'price' Range    | Encoded Value ('price_group') |\n",
    "|---------------------------|--------------------------------|\n",
    "| 0 - 100,000               | 0                              |\n",
    "| 100,001 - 200,000         | 1                              |\n",
    "| 200,001 - 300,000         | 2                              |\n",
    "| 300,001 - 400,000         | 3                              |\n",
    "| 400,001 - 500,000         | 4                              |\n",
    "| 500,001 - 600,000         | 5                              |\n",
    "| 600,001 - 700,000         | 6                              |\n",
    "| 700,001 - 800,000         | 7                              |\n",
    "| 800,001 - 900,000         | 8                              |\n",
    "| 900,001 - 1,000,000       | 9                              |\n",
    "| 1,000,000+                | 10                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_columns(df):\n",
    "    \"\"\"Creates and encodes 'yr_built' and 'price' group columns.\"\"\"\n",
    "    # Define 'yr_built' groups\n",
    "    min_year = df['yr_built'].min()\n",
    "    start_year = min_year if min_year < 1900 else 1899\n",
    "    bins_yr_built = [start_year, 1900, 1920, 1940, 1960, 1980, 2000, 2020]\n",
    "    labels_yr_built = ['<1900', '1900-1920', '1921-1940', '1941-1960', '1961-1980', '1981-2000', '2001-2020']\n",
    "    df['yr_built_group'] = pd.cut(df['yr_built'], bins=bins_yr_built, labels=labels_yr_built)\n",
    "\n",
    "    # Define 'price' groups\n",
    "    bins_price = [0, 100000, 200000, 300000, 400000, 500000, 600000, 700000, 800000, 900000, 1000000, df['price'].max()]\n",
    "    labels_price = ['0-100k', '100k-200k', '200k-300k', '300k-400k', '400k-500k', '500k-600k', '600k-700k', '700k-800k', '800k-900k', '900k-1M', '1M+']\n",
    "    df['price_group'] = pd.cut(df['price'], bins=bins_price, labels=labels_price)\n",
    "\n",
    "    # Encode 'yr_built_group' and 'price_group'\n",
    "    df['yr_built_group_encoded'] = df['yr_built_group'].astype('category').cat.codes\n",
    "    df['price_group_encoded'] = df['price_group'].astype('category').cat.codes\n",
    "\n",
    "# Create and encode group columns\n",
    "create_group_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Drop Unnecessary Rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are dropping several columns from our dataset.\n",
    "\n",
    "1. The 'id' and 'zipcode' columns are being dropped as they are identifiers that do not contribute to the predictive power of our model.\n",
    "2. The 'yr_built_group', 'price_group', and 'date' columns are also dropped as we have already extracted the necessary information from these columns into other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Drop columns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m drop_columns(\u001b[43mdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\"Drops 'id', 'zipcode','yr_built_group', 'price_group', and 'date' columns.\"\"\"\n",
    "    df = df.drop(['id', 'zipcode', 'date', 'yr_built_group', 'price_group'], axis=1)\n",
    "    return df\n",
    "\n",
    "# Drop columns\n",
    "df = drop_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
